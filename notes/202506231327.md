#  Bits of string: compositional data

(ยง) The article [A flexible Bayesian tool for CoDa mixed models](https://link.springer.com/article/10.1007/s11222-024-10427-3) [@Martinez-Minaya_2024] looks promising; it frames compositional count data in a way so that it's possible to fit by using INLA. 

(ยง) Dimension of of log ratio vectors matters. Compare the CLR $(-1, 1, 0)$ with
$(-1, 1, 0, 0, \ldots)$  (a hundred zeros repeated). The former is roughly
$(0.09, 0.67, 0.24)$ in the simplex, while the latter is roughly 
$(0.004, 0.03, 0.01, 0.01, \ldots)$, which is practically uniform. The log 
ratios between coordinates 1 and 2 (or 1 and 3) remain the same however.

(ยง) Suppose we're simulating (count) data where a subcomposition made up of
$n_1$ components is active and the remaining $n_2$ components are inactive. We
could start with centered (or some other) log ratios and set the inactive
variables to all-0, so we have a CLR vector (choose your favorite basis) 
$x = (x_1, \ldots, x_{n_1}, 0, 0, \ldots, 0)$. What size should $x_i$ be?

As the note above suggests this should depend on dimension. We could decide
that on average we want to have some set proportion, $p$, of counts to fall in
the active subcomposition; let's say $p = 1/4$. Then the probabilities (for a
multinomial draw) corresponding to the active parts should sum to $1/4$. Since 
$\text{clr}^{-1}(x) = \mathcal C(e^{x_1}, \ldots ,e^{x_{n_1}}, 1,1, \ldots, 1)$, where $\mathcal C$ is the closure operator, this suggests that on average we want $3\sum e^{x_i} = n_2\ (*)$.

We'll draw independently $X_i \sim N(0, \sigma)$ and choose $\sigma$ so that
$(*)$ above is true on average. 
If $Y \sim N(\mu, \sigma)$ then $\text{E} e^Y = e^{\mu + \sigma^2/2}$ (cf. [this](https://math.stackexchange.com/a/176330/121200)) so if we take the expectation of $(*)$ we get 

$$
3n_1e^{\sigma^2/2} = n_2 \implies \sigma = \sqrt{2\ln\frac{n_2}{3n_1}}.
$$





