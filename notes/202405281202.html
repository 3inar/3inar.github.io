<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>./einar.sh : PCA is OK (Davis-Kahan $\sin \theta$ theorem)</title>

  <meta name="description" content="Various writing, etc.">
  <meta name="author" content="Einar Holsbø">

  <script data-goatcounter="https://elleve.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>

  <link rel="stylesheet" href="/meta/main.css" />
  <link rel="stylesheet" href="/meta/syntax.css" />
</head>
<body>


<div class="menu">
  <a href="/">about</a>
  —
  <a href="/research/">research</a>
  —
  <a href="/talks/">talks</a>
  —
  <a href="/notes/">notes</a>
  —
  <a href="/links/">links</a>
  —
  <a href="/screenshot_garden/">screenshot garden</a>
  —
  <a href="/meta/"><em>meta</em></a>
</div>



<h1 id="pca-is-ok-davis-kahan-sin-theta-theorem">PCA is OK (Davis-Kahan
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>&#8289;</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">\sin \theta</annotation></semantics></math>
theorem)</h1>
<p>Back in 22 there was a bit of a flareup on twitter over <a
href="https://www.nature.com/articles/s41598-022-14395-4">a paper</a>
with the <em>very</em> spicy title</p>
<blockquote>
<p>Principal Component Analyses (PCA)-based findings in population
genetic studies are highly biased and must be reevaluated</p>
</blockquote>
<p>According to the abstract, there are tens (or even hundreds) of
thousands of papers affected by this high bias, and</p>
<blockquote>
<p>PCA results may not be reliable, robust, or replicable as the field
assumes.</p>
</blockquote>
<p>I would love to live in a world where you could get funding to
re-evaluated a hundred thousand articles. Maybe if you say that you’ll
do it by AI?</p>
<p>I skimmed the paper then and honestly I don’t care to re-read it. I’m
just providing some context for this theorem I know of.</p>
<h2 id="the-davis-kahan-theorem">The Davis-Kahan theorem</h2>
<p>It is certainly true that if you take two random samples from the
same population and do a PCA on both of them, you’re not likely to get
two exactly identical sets of principal component vectors. Or if you
take a single sample from a population you’re not likely to get the
exact true population principal components back. However there is a
theorem that if the “eigengap” (described below) is large enough, the
top principal components are likely to span the right space.</p>
<p><img src="img/20240528_eigengap.png" style="width:80.0%" /></p>
<p>PCA is an eigenvalue decomposition of a symmetric matrix (the
covariance matrix of your data). Suppose for simplicity we’re working in
two dimensions and let our covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
be a symmetric matrix with the eigenvalues
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>λ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\lambda_1 &gt; \lambda_2</annotation></semantics></math>.
In PCA terms these correspond to the “variance explained” values of your
scree plot. The <em>eigengap</em> between these two eigenvalues is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mo>=</mo><msub><mi>λ</mi><mn>1</mn></msub><mo>−</mo><msub><mi>λ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\delta = \lambda_1 - \lambda_2</annotation></semantics></math>.
Now let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">u_1, u_2</annotation></semantics></math>
be the corresponding eigenvectors to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\lambda_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\lambda_2</annotation></semantics></math>.
These are the true population principal components.</p>
<p>We do a random sample and get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">A + E</annotation></semantics></math>,
a perturbation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
(still symmetric), meaning it’s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
plus some noise. We don’t know how much noise is in there. Suppose
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>2</mn></msub><annotation encoding="application/x-tex">v_2</annotation></semantics></math>
are the eigenvectors of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">A+E</annotation></semantics></math>:
how reliable/robust/replicable is an inference based on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math>
in place of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>1</mn></msub><annotation encoding="application/x-tex">u_1</annotation></semantics></math>?</p>
<p>The DK theorem informs us that if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is the angle between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>1</mn></msub><annotation encoding="application/x-tex">u_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math>
we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>&#8289;</mo><mi>θ</mi><mo>≤</mo><mfrac><mrow><msqrt><mn>2</mn></msqrt><msub><mrow><mo stretchy="true" form="prefix">‖</mo><mi>E</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mi>F</mi></msub></mrow><mi>δ</mi></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
\sin \theta \leq \frac{\sqrt{2} \lVert E \rVert_F}{\delta},
</annotation></semantics></math></p>
<p>with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mo stretchy="true" form="prefix">‖</mo><mi>⋅</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mi>F</mi></msub><annotation encoding="application/x-tex">\lVert \cdot \rVert_F</annotation></semantics></math>
the <a
href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius
norm</a>.</p>
<p>In other words, the angle between our observed vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math>
and the hidden true vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>1</mn></msub><annotation encoding="application/x-tex">u_1</annotation></semantics></math>
is smaller than the scale of the added noise divided by the size of the
eigengap. Multiplied by an annoying constant. We get a vector close to
(ie with small angle to) the truth if we have little noise or a large
eigengap (or, God willing, both).</p>
<p>This extends to several dimensions where instead of looking at the
angle between two vectors we look at the angle between subspaces. The
space spanned by the top
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
principal components is robust if there is a large gap between the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-th
and the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k+1</annotation></semantics></math>th
eigenvalues.</p>
<h2 id="bias">Bias?</h2>
<p>I don’t believe that a hundred thousand articles are <em>biased</em>,
in the technical sense, from use of PCA, but I 100% believe that many
many articles have large noise and small eigengaps. But you can’t really
blame PCA for noisy data.</p>
<h2 id="references">References</h2>
<p>Two nice blog posts on this sort of thing:</p>
<ul>
<li><a href="http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/"
class="uri">http://yueqicao.top/2021/01/12/Davis-Kahan-s-Theorem/</a></li>
<li><a
href="http://yueqicao.top/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/"
class="uri">http://yueqicao.top/2021/04/09/Random-Perturbation-to-Low-Rank-Matrices/</a></li>
</ul>
<p>Original paper of D and K (did not read):</p>
<ul>
<li><a href="https://epubs.siam.org/doi/pdf/10.1137/0707001"
class="uri">https://epubs.siam.org/doi/pdf/10.1137/0707001</a></li>
</ul>
<p>There is a paper called <em>A useful variant of the Davis–Kahan
theorem for statisticians</em>, which has a promising title.</p>
<p>On measuring the angle between subspaces:</p>
<ul>
<li><a
href="https://mathoverflow.net/questions/102153/angle-between-subspaces"
class="uri">https://mathoverflow.net/questions/102153/angle-between-subspaces</a></li>
</ul>




<small>this file last touched 2024.05.28</small></body>
</html>
