---
title: "Neural network overengineering"
author: "Einar"
date: 2016-08-24
#categories: ["R"]
tags: ["neural_networks", "naive_bayes", "wine"]
math: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In my idle hours (ie when I should have been writing a paper), I came across this
blog post where the author <a href="https://medium.com/autonomous-agents/how-to-train-your-neuralnetwork-for-wine-tasting-1b49e0adff3a#.l2fhfa6hr">trained a neural netwok in the fine art of wine tasting</a>.
I am basically deep down a very lazy person, and from what little I understand of
NNs, there are quite a few knobs to fiddle with, which really who has the time
with all this untasted wine floating about? 5.56 hidden nodes? 600 epochs?
Feature scaling? Honestly! What I need is something <em>simple</em>.</p>
<p>Now for simplicity you can hardly beat good old Naive Bayes.
For those of us who forget, Naive Bayes is to assume that the predictors
<span class="math inline">\(x_i\)</span> in an observation <span class="math inline">\(\mathbf x = [x_1, x_1, \ldots, x_p]\)</span> are independent
so that the probability of a given sample, <span class="math inline">\(\mathbf x\)</span>, belonging to a given class,
<span class="math inline">\(C\)</span>, is <span class="math inline">\(p(C | X = \mathbf x) \propto p(C)\prod_{i=1}^p p(x_i|C)\)</span>. The
class-conditional densities, <span class="math inline">\(p(x_i | C)\)</span>, are usually fit as
maximum-likelihood-estimate Gaussians.</p>
<p>Anyway, <a href="https://archive.ics.uci.edu/ml/datasets/Wine">get the data from UCI</a>
and have a go. Follow this simple guide and you too will have
created a wine-tasting AI to threaten the job security of sommeliers
everywhere. First let’s load the data and have a look at it.</p>
<pre class="r"><code>set.seed(220816) # for reproducibility 

# load data, first column in file is class
wino &lt;- read.csv(&quot;data/wine.data&quot;, header=F)
x &lt;- as.matrix(wino[, -1])
y &lt;- as.factor(wino[, 1])

# from wine.names
colnames(x) &lt;- c(&quot;Alcohol&quot;, &quot;Malic acid&quot;, &quot;Ash&quot;, &quot;Alcalinity of ash&quot;,
  &quot;Magnesium&quot;, &quot;Total phenols&quot;, &quot;Flavanoids&quot;, &quot;Nonflavanoid phenols&quot;,
  &quot;Proanthocyanins&quot;, &quot;Color intensity&quot;, &quot;Hue&quot;, &quot;OD280/OD315 of diluted wines&quot;,
  &quot;Proline&quot;)

# set aside 0.35 of the samples for independent test set as per other blog
test_idx &lt;- sample(1:length(y), ceiling(length(y)*0.35))</code></pre>
<p>To get a feel for the data we’ll plot the class-conditional densities of
the different predictors. We exclude the training samples, of course,
so as not to fool ourselves. The code for this plot is kind of ugly and
unintersting so I’ll just hide it.</p>
<p><img src="public/blog/2016-08-24-neural-network-overengineering_files/figure-html/density_estimates-1.png" width="672" /></p>
<p>Doesn’t look too bad. Some of these don’t look particularly Gaussian,
but let’s just say that we’re doing smoothing. Now to fit our model and
look at how it classifies the different wines.</p>
<pre class="r"><code>library(e1071) # provides naiveBayes()
nbfit &lt;- naiveBayes(x[-test_idx, ], y[-test_idx])

pred &lt;- predict(nbfit, x[test_idx, ], type=&quot;class&quot;)
confusion &lt;- table(pred, y[test_idx]); confusion</code></pre>
<pre><code>##     
## pred  1  2  3
##    1 25  0  0
##    2  0 25  0
##    3  0  0 13</code></pre>
<p>That’s a pretty friendly-looking confusion matrix; we only have a single
misclassification. Accuracy is the number of correctly classified observations
over the total number of observations classified:</p>
<pre class="r"><code>acc_est &lt;- sum(diag(confusion))/sum(confusion); acc_est</code></pre>
<pre><code>## [1] 1</code></pre>
<p>So that’s pretty good. Since I believe in uncertainty estimates, we’ll do a
bootstrapped quantile confidence interval. I estimate by 100% guessing that
5000 bootstrap samples is enough. Much more than enough probably.</p>
<pre class="r"><code>library(plyr)
boots &lt;- raply(5000, sample((1:nrow(x))[-test_idx], replace=T))

stats &lt;- aaply(boots, 1, function(bsindex) {
  nbfit &lt;- naiveBayes(x[bsindex, ], y[bsindex])

  pred &lt;- predict(nbfit, x[test_idx, ], type=&quot;class&quot;)
  confusion &lt;- table(pred, y[test_idx])
  sum(diag(confusion))/sum(confusion)
})

quantile(stats, c(0.025, 0.975))</code></pre>
<pre><code>##     2.5%    97.5% 
## 0.968254 1.000000</code></pre>
<p>The results are in: our intelligent wine tasting machine has an estimated accuracy
of 1 with a 0.95 CI of
[0.97, 1].
That’s good enough for me, and what’s more I never have to drink wine alone again.</p>
